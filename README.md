# ğŸ”® Next Word Predictor  
### LSTM-based Deep Learning NLP Web Application

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange)
![Flask](https://img.shields.io/badge/Flask-Web%20App-green)
![NLP](https://img.shields.io/badge/NLP-LSTM-purple)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)

---

## ğŸ“Œ Project Overview

**Next Word Predictor** is an end-to-end **Natural Language Processing (NLP)** web application that predicts the **most probable next word** in a sentence using a **Long Short-Term Memory (LSTM)** neural network.

This project demonstrates **real-world deep learning deployment**, combining:
- Sequential text modeling  
- Trained LSTM networks  
- Flask-based backend serving  
- Interactive web-based UI  

After experimenting with **3 different LSTM architectures across 2 datasets**, the final selected model achieved a **validation accuracy of ~72%**, making it both reliable and production-ready for educational and prototype use cases.

---

## ğŸ¯ Problem Statement

Language is sequential by nature. Traditional machine learning models fail to capture long-term dependencies in text.  
This project solves that by leveraging **LSTM networks**, which are specifically designed to remember context across long sequences.

**Goal:**  
> Given a partial sentence, predict the most contextually relevant next word.

---

## âœ¨ Key Features

- âœ… **Context-Aware Predictions** using LSTM  
- âš¡ **Real-Time Inference** through Flask API  
- ğŸ§  **Trained Deep Learning Model** (Saved & Reloaded)  
- ğŸ”„ **Consistent Preprocessing** using saved Tokenizer & padding length  
- ğŸ¨ **Clean Web Interface** using HTML & CSS  
- ğŸ“¦ **Production-Ready Structure** with reusable artifacts  

---

## ğŸ› ï¸ Tech Stack

### ğŸ”¹ Machine Learning & NLP
- **TensorFlow / Keras** â€“ Model building, training & inference  
- **LSTM (RNN)** â€“ Sequential language modeling  
- **NumPy** â€“ Numerical computations  
- **Pickle** â€“ Serialization of tokenizer and metadata  

### ğŸ”¹ Backend & Web
- **Flask** â€“ Lightweight backend server  
- **HTML5 & CSS3** â€“ Frontend interface  

---

## ğŸ§  Model Architecture & Approach

### 1ï¸âƒ£ Text Preprocessing Pipeline
- **Tokenization**  
  Converts words into integer indices using a trained `Tokenizer`.
- **Sequence Creation**  
  Builds n-gram sequences to learn sentence progression.
- **Padding**  
  Uses a fixed `max_len_x` to normalize input length.

### 2ï¸âƒ£ LSTM Neural Network
- Embedding Layer â†’ LSTM Layer(s) â†’ Dense Softmax Output  
- Designed to:
  - Capture long-term dependencies  
  - Handle variable-length text  
  - Avoid vanishing gradient issues common in vanilla RNNs  

### 3ï¸âƒ£ Model Experimentation
| Model Version | Dataset | Result |
|--------------|--------|--------|
| Model 1 | Dataset A | Underfitting |
| Model 2 | Dataset B | Overfitting |
| **Model 3 (Final)** | Combined | **~72% Accuracy** âœ… |

---

## ğŸ“‚ Project Structure

```bash
Next-Word-Predictor/
â”‚
â”œâ”€â”€ Data_set/
â”‚   â”œâ”€â”€ processed_quotes_cleaned.csv   # Cleaned & preprocessed dataset
â”‚   â””â”€â”€ quote_dataset.csv              # Original raw text dataset
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ lstm_model.h5                  # Trained LSTM model (version 1)
â”‚   â”œâ”€â”€ lstm_model2.h5                 # Trained LSTM model (version 2)
â”‚   â”œâ”€â”€ lstm_model3.h5                 # Best-performing LSTM model (final)
â”‚   â”œâ”€â”€ max_len_X.pkl                  # Serialized max sequence length
â”‚   â””â”€â”€ tokenizer.pkl                  # Serialized tokenizer object
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css                      # Frontend UI styling
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html                     # Web application interface
â”‚
â”œâ”€â”€ app.py                             # Flask application entry point
â”œâ”€â”€ requirements.txt                  # Python dependencies
â””â”€â”€ README.md                          # Project documentation


âš™ï¸ Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone https://github.com/yourusername/next-word-predictor.git
cd next-word-predictor

2ï¸âƒ£ Create Virtual Environment (Recommended)
python -m venv venv


Activate:

Windows

venv\Scripts\activate


Linux / macOS

source venv/bin/activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Run the Application
python app.py

5ï¸âƒ£ Access the Web App

Open your browser and go to:

http://127.0.0.1:5000/

ğŸ“¦ Major Dependencies

flask

tensorflow

numpy

pickle-mixin

(See requirements.txt for full list)

ğŸ“¸ Screenshots

ğŸ“Œ Add screenshots of the UI here to improve visual appeal and recruiter impact.

ğŸš€ Future Enhancements

ğŸ”® Top-k / Top-n word predictions

ğŸ“ˆ Beam search for better sentence generation

ğŸ§  Transformer-based model (GPT-style)

ğŸŒ Multi-language support

â˜ï¸ Cloud deployment (AWS / Render / Hugging Face Spaces)

ğŸ§‘â€ğŸ’» Author

Bhautik Gondaliya
Aspiring Data Scientist | Machine Learning & NLP Enthusiast

This project reflects hands-on experience in Deep Learning, NLP pipelines, Flask deployment, and model lifecycle management.

â­ Acknowledgments

TensorFlow & Keras Documentation

NLP research & sequence modeling concepts
